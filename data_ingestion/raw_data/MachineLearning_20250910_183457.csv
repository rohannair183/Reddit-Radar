id,title,selftext,subreddit,author,score,upvote_ratio,num_comments,created_utc,url,permalink,link_flair_text,is_self,over_18,spoiler,stickied,locked,distinguished,retrieved_at
1n67lft,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

\--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

\--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",MachineLearning,AutoModerator,15,0.89,25,1756779330.0,https://www.reddit.com/r/MachineLearning/comments/1n67lft/d_selfpromotion_thread/,https://reddit.com/r/MachineLearning/comments/1n67lft/d_selfpromotion_thread/,Discussion,True,False,False,True,False,,2025-09-10T22:30:37.758596+00:00
1n4jdo7,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",MachineLearning,AutoModerator,13,0.79,2,1756607434.0,https://www.reddit.com/r/MachineLearning/comments/1n4jdo7/d_monthly_whos_hiring_and_who_wants_to_be_hired/,https://reddit.com/r/MachineLearning/comments/1n4jdo7/d_monthly_whos_hiring_and_who_wants_to_be_hired/,Discussion,True,False,False,True,False,,2025-09-10T22:30:38.763608+00:00
1ndo5md,[D]NVIDIA Blackwell Ultra crushes MLPerf,"NVIDIA dropped MLPerf results for Blackwell Ultra yesterday. 5√ó throughput on DeepSeek-R1, record runs on Llama 3.1 and Whisper, plus some clever tricks like FP8 KV-cache and disaggregated serving. The raw numbers are insane.

But I wonder though . If these benchmark wins actually translate into lower real-world inference costs.

In practice, workloads are bursty. GPUs sit idle, batching only helps if you have steady traffic, and orchestration across models is messy. You can have the fastest chip in the world, but if 70% of the time it‚Äôs underutilized, the economics don‚Äôt look so great to me. IMO",MachineLearning,pmv143,10,0.78,1,1757533191.0,https://www.reddit.com/r/MachineLearning/comments/1ndo5md/dnvidia_blackwell_ultra_crushes_mlperf/,https://reddit.com/r/MachineLearning/comments/1ndo5md/dnvidia_blackwell_ultra_crushes_mlperf/,Discussion,True,False,False,False,False,,2025-09-10T22:30:39.768590+00:00
1ndrpvu,[N] Delta Flow | Generating buildable digital twins in minutes,"Delta Flow, an architecture, engineering, and construction (AEC) deep-tech company, is fundamentally accelerating the pre-construction timeline.

https://www.delta-flow.ai/

In a market where architectural design takes months, Delta Flow's AI platform generates complete, buildable digital twins in minutes. Their recently released beta is already producing tangible deliverables for construction, including:

üìú PDF blueprints
üè° Intelligent 3D models
üó∫ Site plan feasibility & materials lists

This isn't just a design tool; it's a powerful engine for building information modeling (BIM) automation that gives them a significant competitive edge in the multi-trillion-dollar construction industry. Plug and Play and Hatcher+ back them and have a clear roadmap, with their next major feature being the generation of advanced 3D footprints. In California, they have already secured local contractors as paying customers.",MachineLearning,Commercial_Sample389,0,0.5,0,1757541945.0,https://www.reddit.com/r/MachineLearning/comments/1ndrpvu/n_delta_flow_generating_buildable_digital_twins/,https://reddit.com/r/MachineLearning/comments/1ndrpvu/n_delta_flow_generating_buildable_digital_twins/,News,True,False,False,False,False,,2025-09-10T22:30:40.769021+00:00
1ndrm0y,Delta Flow | Generating buildable digital twins in minutes [D],"Delta Flow, an architecture, engineering, and construction (AEC) deep-tech company, is fundamentally accelerating the pre-construction timeline.

https://www.delta-flow.ai/

In a market where architectural design takes months, Delta Flow's AI platform generates complete, buildable digital twins in minutes. Their recently released beta is already producing tangible deliverables for construction, including:

üìú PDF blueprints
üè° Intelligent 3D models
üó∫ Site plan feasibility & materials lists

This isn't just a design tool; it's a powerful engine for building information modeling (BIM) automation that gives them a significant competitive edge in the multi-trillion-dollar construction industry.",MachineLearning,Commercial_Sample389,0,0.5,0,1757541670.0,https://www.reddit.com/r/MachineLearning/comments/1ndrm0y/delta_flow_generating_buildable_digital_twins_in/,https://reddit.com/r/MachineLearning/comments/1ndrm0y/delta_flow_generating_buildable_digital_twins_in/,Discussion,True,False,False,False,False,,2025-09-10T22:30:41.769028+00:00
1ndaesz,[D] SOTA modern alternative to BertScore?,"Hi everyone,  
I‚Äôm looking for an embedding-based metric to score text generation. BertScore is great, but it‚Äôs a bit outdated. Could you suggest some modern state-of-the-art alternatives?

",MachineLearning,Soft-Possibility2929,8,0.79,4,1757499778.0,https://www.reddit.com/r/MachineLearning/comments/1ndaesz/d_sota_modern_alternative_to_bertscore/,https://reddit.com/r/MachineLearning/comments/1ndaesz/d_sota_modern_alternative_to_bertscore/,Discussion,True,False,False,False,False,,2025-09-10T22:30:42.773986+00:00
1ndajmq,[D] Questions on Fairness and Expectations in Top-Tier Conference Submissions,"Hello everyone,

I know that in this community there are many experienced researchers and even reviewers for top-tier conferences. As a young researcher, I sincerely hope to learn from your perspectives and get some clarity on a few concerns I‚Äôve been struggling with.

**My first question:**  
Does a research paper always need to achieve *state-of-the-art (SOTA)* results‚Äîoutperforming every existing method‚Äîto be accepted at an A\* conference? I often feel that so many published papers present dazzling results, making it nearly impossible for newcomers to surpass them.

**My second question, about fairness and accuracy in comparisons:**  
When evaluating a new method, is it acceptable to compare primarily against the most ‚Äúrelated,‚Äù ‚Äúsimilar,‚Äù or ‚Äúsame-family‚Äù methods rather than the absolute SOTA? For example:

* If I make a small modification to the Bagging procedure in Random Forest, would it be fair to compare only against other Bagging-based forests, rather than something fundamentally different like XGBoost (which is boosting-based)?
* Similarly, if I improve a variant of SVM, is it reasonable to compare mainly with other margin-based or kernel methods, instead of tree-based models like Decision Trees?

I understand that if my method only beats some similar baselines but does not surpass the global best-performing method, reviewers might see it as ‚Äúmeaningless‚Äù (since people naturally gravitate toward the top method). Still, I‚Äôd like to hear your thoughts: from an experienced researcher‚Äôs point of view, what is considered fair and convincing in such comparisons?

Thank you very much in advance for your time and advice.",MachineLearning,Feuilius,3,0.62,6,1757500247.0,https://www.reddit.com/r/MachineLearning/comments/1ndajmq/d_questions_on_fairness_and_expectations_in/,https://reddit.com/r/MachineLearning/comments/1ndajmq/d_questions_on_fairness_and_expectations_in/,Discussion,True,False,False,False,False,,2025-09-10T22:30:43.774213+00:00
1ndbzb7,[D] ICCV 2025 registration,"Two years ago at Paris I had a workshop paper, I purchased the workshop entrance ticket, everything is okay.

This year I have done the same and now I am receiving emails saying only a full conference entrance is considered an author registration for a workshop paper. 

I did see the website is slightly different this year but still‚Ä¶ the code of conduct did not explain this clearly, does anyone have better insights for me?",MachineLearning,ScaryCommission7829,2,1.0,0,1757504917.0,https://www.reddit.com/r/MachineLearning/comments/1ndbzb7/d_iccv_2025_registration/,https://reddit.com/r/MachineLearning/comments/1ndbzb7/d_iccv_2025_registration/,Discussion,True,False,False,False,False,,2025-09-10T22:30:44.779259+00:00
1ncu0na,[D] IJCNLP-AACL 2025: Paper Reviews (ARR July 2025 Cycle),"The ARR July cycle reviews for AACL-IJCNLP 2025 just dropped.  
Feel free to share your thoughts and feelings! How did you do?",MachineLearning,Starscream-11813,19,0.96,12,1757449397.0,https://www.reddit.com/r/MachineLearning/comments/1ncu0na/d_ijcnlpaacl_2025_paper_reviews_arr_july_2025/,https://reddit.com/r/MachineLearning/comments/1ncu0na/d_ijcnlpaacl_2025_paper_reviews_arr_july_2025/,Discussion,True,False,False,False,False,,2025-09-10T22:30:45.781731+00:00
1ndgutf,[D] Is lowering error rates more impactful than higher benchmarks?,"The biggest impact I noticed when moving from GPT-4.1 to GPT-5 wasn‚Äôt the benchmark score improvements. It was the reduction in error rate.

When solving harder tasks, the difference feels very practical: far fewer retries, much less wasted context, lower time and token cost.

It makes me wonder:

Do we sometimes conflate ‚Äúability‚Äù with ‚Äúreliability‚Äù?  
Could lowering error rates be a more important research direction than pushing benchmarks higher?  
How do researchers here think about measuring and prioritizing this tradeoff?

I‚Äôd love to hear from people who approach this more systematically.  
(For context: I‚Äôm not a researcher myself, but a founder and heavy LLM user sharing observations.)



# (Edit)

Quick clarification: when I say ""error rate"" I mean real-world, user-facing error rate ‚Äî i.e., how often a model's output is usable for the task without corrective interaction, not just benchmark accuracy.

Benchmarks are useful and I'm not dismissing them, but they don't always capture the cost of hallucinations: retries, lost context, wasted time, wrong actions. In practice, things like:

task success rate (end-to-end), average retries or clarification turns, human correction time or token cost, false-positive hallucinations vs refusal rate.

These often matter more to whether I can actually reach a goal efficiently.

What I'd really like to hear are perspectives on how the field should think about this distinction. Should reliability (in the sense of fewer misleading outputs) be treated as a separate axis of progress from raw benchmark scores?",MachineLearning,RM-Li,0,0.43,12,1757517056.0,https://www.reddit.com/r/MachineLearning/comments/1ndgutf/d_is_lowering_error_rates_more_impactful_than/,https://reddit.com/r/MachineLearning/comments/1ndgutf/d_is_lowering_error_rates_more_impactful_than/,Discussion,True,False,False,False,False,,2025-09-10T22:30:46.786693+00:00
1ndgsxc,[D] Help!!! TorchCodec error when loading audio dataset with ü§ódatasets,"I‚Äôm trying to use the audio dataset¬†`Sunbird/urban-noise-uganda-61k`¬†with ü§ódatasets.

After loading the dataset, when I try to access an entry like this:

    dataset = load_dataset(""Sunbird/urban-noise-uganda-61k"", ""small"")
    sample = dataset['train'][0]

I get the following error:

    RuntimeError: Could not load libtorchcodec. 
    Likely causes: 
    1. FFmpeg is not properly installed in your environment. We support versions 4, 5, 6 and 7. 
    2. The PyTorch version (2.8.0+cpu) is not compatible with this version of TorchCodec. Refer to the version compatibility table: https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec. 
    3. Another runtime dependency; see exceptions below.
    
    The following exceptions were raised as we tried to load libtorchcodec: 
    [start of libtorchcodec loading traceback] 
    FFmpeg version 7: Could not find module 'D:\Projects\UrbanNoiseClassifier\.venv\Lib\site-packages\torchcodec\libtorchcodec_core7.dll' (or one of its dependencies). Try using the full path with constructor syntax. 
    FFmpeg version 6: Could not find module 'D:\Projects\UrbanNoiseClassifier\.venv\Lib\site-packages\torchcodec\libtorchcodec_core6.dll' (or one of its dependencies). Try using the full path with constructor syntax. 
    FFmpeg version 5: Could not find module 'D:\Projects\UrbanNoiseClassifier\.venv\Lib\site-packages\torchcodec\libtorchcodec_core5.dll' (or one of its dependencies). Try using the full path with constructor syntax. 
    FFmpeg version 4: Could not find module 'D:\Projects\UrbanNoiseClassifier\.venv\Lib\site-packages\torchcodec\libtorchcodec_core4.dll' (or one of its dependencies). Try using the full path with constructor syntax.
    [end of libtorchcodec loading traceback]

# What I‚Äôve tried so far:

1. Installed¬†**FFmpeg v7**¬†and added it to¬†`PATH`.
2. Installed¬†**PyTorch v2.8.0+cpu**¬†and matched it with¬†**TorchCodec v0.7**.
3. Verified that the required¬†`.dll`¬†files exist.

From what I understand, the audio files are decoded on the fly using¬†**TorchCodec**, and the issue seems to be with its dependencies.

Has anyone faced this issue before? Any ideas on how to resolve the¬†`libtorchcodec`¬†loading problem?",MachineLearning,pi_ndi,0,0.33,0,1757516941.0,https://www.reddit.com/r/MachineLearning/comments/1ndgsxc/d_help_torchcodec_error_when_loading_audio/,https://reddit.com/r/MachineLearning/comments/1ndgsxc/d_help_torchcodec_error_when_loading_audio/,Discussion,True,False,False,False,False,,2025-09-10T22:30:47.791607+00:00
1ncq2yg,[D] Graphrag  pipeline that runs entirely locally with ollama and has full source attribution,"I built a Graph RAG pipeline (VeritasGraph) that runs entirely locally with Ollama (Llama 3.1) and has full source attribution.

I've been deep in the world of local RAG and wanted to share a project I built, VeritasGraph, that's designed from the ground up for private, on-premise use with tools we all love.

My setup uses Ollama with llama3.1 for generation and nomic-embed-text for embeddings. The whole thing runs on my machine without hitting any external APIs.

The main goal was to solve two big problems:

Multi-Hop Reasoning: Standard vector RAG fails when you need to connect facts from different documents. VeritasGraph builds a knowledge graph to traverse these relationships.

Trust & Verification: It provides full source attribution for every generated statement, so you can see exactly which part of your source documents was used to construct the answer.

One of the key challenges I ran into (and solved) was the default context length in Ollama. I found that the default of 2048 was truncating the context and leading to bad results. The repo includes a Modelfile to build a version of llama3.1 with a 12k context window, which fixed the issue completely.

The project includes:

The full Graph RAG pipeline.

A Gradio UI for an interactive chat experience.

A guide for setting everything up, from installing dependencies to running the indexing process.

GitHub Repo with all the code and instructions: https://github.com/bibinprathap/VeritasGraph

I'd be really interested to hear your thoughts, especially on the local LLM implementation and prompt tuning. I'm sure there are ways to optimize it further.

Thanks!  ",MachineLearning,BitterHouse8234,15,0.84,2,1757440809.0,https://www.reddit.com/r/MachineLearning/comments/1ncq2yg/d_graphrag_pipeline_that_runs_entirely_locally/,https://reddit.com/r/MachineLearning/comments/1ncq2yg/d_graphrag_pipeline_that_runs_entirely_locally/,Discussion,True,False,False,False,False,,2025-09-10T22:30:48.796598+00:00
1ncdt5o,[P] Implementation and ablation study of the Hierarchical Reasoning Model (HRM): what really drives performance?,"I recently implemented the [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734) (HRM) for educational purposes and applied it to a simple pathfinding task. You can watch the model solve boards step by step in the generated animated GIF.

HRM is inspired by multi-timescale processing in the brain: a slower H module for abstract planning and a faster L module for low-level computation, both based on self-attention. HRM is an attempt to model reasoning in latent space.

To understand a bit better what drives the performance I ran a small ablation study. Key findings (full results in the README):

* The biggest driver of performance (both accuracy and refinement ability) is training with more segments (outer-loop refinement), not architecture.
* The two-timescale H/L architecture performs about the same as a single-module trained with BPTT.
* Notably, H/L still achieves good performance/refinement without full BPTT, which could mean cheaper training.

Repo: [https://github.com/krychu/hrm](https://github.com/krychu/hrm)

This is of course a limited study on a relatively simple task, but I thought the results might be interesting to others exploring reasoning models.

The findings line up with the ARC Prize team's analysis: [https://arcprize.org/blog/hrm-analysis](https://arcprize.org/blog/hrm-analysis)

Below two examples of refinement in action: early steps explore solution with rough guesses, later steps make smaller and smaller corrections until the full path emerges:

[20x20 board](https://i.redd.it/i1qi4l2vs3of1.gif)

[30x30 board](https://i.redd.it/j6fpueovs3of1.gif)

",MachineLearning,krychu,57,1.0,7,1757408387.0,https://www.reddit.com/r/MachineLearning/comments/1ncdt5o/p_implementation_and_ablation_study_of_the/,https://reddit.com/r/MachineLearning/comments/1ncdt5o/p_implementation_and_ablation_study_of_the/,Project,True,False,False,False,False,,2025-09-10T22:30:49.801662+00:00
1ncemg3,[D] What‚Äôs the most frustrating ‚Äústuck‚Äù moment you‚Äôve faced in an ML project?,"Curious about community experience: what‚Äôs the most painful ‚Äòstuck‚Äô moment you‚Äôve faced in an ML project (convergence, dataset issues, infra)?  
How did you eventually move past it, or did you abandon the attempt? Would be great to hear real war stories beyond published papers.",MachineLearning,ExtentBroad3006,26,0.91,27,1757411548.0,https://www.reddit.com/r/MachineLearning/comments/1ncemg3/d_whats_the_most_frustrating_stuck_moment_youve/,https://reddit.com/r/MachineLearning/comments/1ncemg3/d_whats_the_most_frustrating_stuck_moment_youve/,Discussion,True,False,False,False,False,,2025-09-10T22:30:50.806654+00:00
1ncceqw,[D] Best ocr as of now,"I want to know which ocr has high accuracy and consumes less time for the extraction of data for given input images (especially tables), anything which works better than paddleocr?",MachineLearning,Coffeee_addictt,19,0.92,8,1757402626.0,https://www.reddit.com/r/MachineLearning/comments/1ncceqw/d_best_ocr_as_of_now/,https://reddit.com/r/MachineLearning/comments/1ncceqw/d_best_ocr_as_of_now/,Discussion,True,False,False,False,False,,2025-09-10T22:30:51.807196+00:00
1nc5jb5,"[R] LLMs play a cooperative card game, coordination without communication","One of my favorite card games is called The Crew, which is a trick-taking game (like hearts) but cooperative. There's no table talk allowed, players have to coordinate silently (with limited options for in-game communication) - figuring out what their teammates are doing and why, and what they need to do to work together. I wondered what SOTA LLMs would do if you asked them to play. To make this work, I implemented a backend for the game logic and structured outputs so models play by submitting moves and reasoning at each turn. 

Originally I wanted to re-create the 50 mission campaign, but models were so spotty on  mission 1 (the simplest possible mission) that I stuck to mission 1 and experimented with different configurations instead. I ran 8 OpenAI models on 10 different versions, ranging from very easy (random chance gets you there 2/3rds of the time) to very hard (random chance succeeds 0.5%), and gave each model ten trials on each mission.

What I've found out:

\* Smaller models struggle both with gameplay, and with understanding their role on the team. In these missions, a designated player (the commander) has to win a designated card. But these models hate having to lose a trick for the sake of their teammate, even when that's how they win the game.

[This does not \\""help him secure the win and fulfill his task.\\"" It loses the game.](https://preview.redd.it/3lqyqf3tg1of1.png?width=2030&format=png&auto=webp&s=b57c0a46fee169e14dbf6fc0cda107024a11a59e)

\* GPT-4o-mini (worst model so far) plays randomly on easy setups and worse than randomly on harder ones. GPT-4o-mini in particular loses the game in the first turn almost 90% of the time in harder setups with GPT-5-nano and GPT-4.1-mini are close behind at 60-70%. 

[GREEN 1 is the lowest GREEN card in the game, so playing it straight away actually guarantees immediate failure.](https://preview.redd.it/fx5jqyhug1of1.png?width=2046&format=png&auto=webp&s=da5d4abb5a7fcd4c1e8ee42c09d7acfb4a7ba5dc)

\* GPT-5 is self-aware enough to avoid the ""losing on the very first turn"" error, but actually did it on purpose once as a deliberate suicide when it saw that it couldn't win the game on the very first turn.

[There are multiple turns in the game!](https://preview.redd.it/91qnnfuvg1of1.jpg?width=1900&format=pjpg&auto=webp&s=ebc98a3fbf4381c4a95f7e96ec2fa96f8e84692f)

\* The harder missions - which require coordination across multiple turns - absolutely cook the smaller models with <10% win rates. Only GPT-5 is beating random chance on the harder missions (73% GPT-5 vs 4% random) 

\* GPT-5 also found optimal 1-trick solutions to a couple of setups I thought required at least two tricks. Oops. So in a sense, we're above human performance in some areas.

\* ...But most of the time, GPT-5 generally screwed around for 3 or more tricks in puzzles it could have solved in 1. This is like solving a mate in one chess puzzle in 3 moves. It's not losing, but it's not exactly showing a mastery of the game.

\* The lack of goal-oriented behavior (or risk-averse hesitation) on GPT-5's part means that GPT-5-mini actually performs better if we count speed (number of turns) to win as criteria and grade on optimal play (winning in the least number of turns, rather than just winning.)

I published the repo and did a write-up with some graphs and demos here: [https://ekkarpinski.github.io/LLMCrew/](https://ekkarpinski.github.io/LLMCrew/)

",MachineLearning,ekkarpinski,42,0.91,10,1757380091.0,https://www.reddit.com/r/MachineLearning/comments/1nc5jb5/r_llms_play_a_cooperative_card_game_coordination/,https://reddit.com/r/MachineLearning/comments/1nc5jb5/r_llms_play_a_cooperative_card_game_coordination/,Research,True,False,False,False,False,,2025-09-10T22:30:52.812163+00:00
1nd63b0,[D] Document Forgery using GenAI,"Hi there,

Curious as to how the world is dealing with a lot of GenAI created images and documents that are sometimes being used as proof for some sort of claims -- basically lack of integrity verification methods.

Let's assume a scenario where a business owner sends an invoice to their customers by uploading it in web-portal. But there's possibility that the invoice might be AI generated/tampered in order to mess up the original charges or some amount. And the web-portal needs a solutions for this.

A plausible solution by google for such problems is their watermarking tech for AI generated content:¬†[https://deepmind.google/science/synthid/](https://deepmind.google/science/synthid/)

Would like to know your insights on this.

Thanks.",MachineLearning,r0075h3ll,0,0.18,5,1757483133.0,https://www.reddit.com/r/MachineLearning/comments/1nd63b0/d_document_forgery_using_genai/,https://reddit.com/r/MachineLearning/comments/1nd63b0/d_document_forgery_using_genai/,Discussion,True,False,False,False,False,,2025-09-10T22:30:53.815836+00:00
1ncrkpp,[D] Negative R¬≤ on unseen dataset despite good train/test performance,"I am working on a regression problem where I predict Pavement Condition Index (PCI) values from multi-sensor time-series data collected in the same region and under the same conditions. I have multiple sets of data from the same collection process, where I use some sets for training and testing and keep the remaining ones for evaluating generalization. Within the training and testing sets, the model performs well, but when I test on the held-out dataset from the same collection, the R¬≤ value often becomes negative , even though the mean absolute error and root mean square error remain reasonable. I have experimented with several feature engineering strategies, including section-based, time-based, and distance-based windowing, and I have tried using raw PCI data as well. I also tested different window lengths and overlap percentages, but the results remain inconsistent. I use the same data for a classification task, the models perform very well and generalize properly, yet for PCI regression, the generalization fails despite using the same features and data source. In some cases, removing features like latitude, longitude, or timestamps caused performance to drop significantly, which raises concerns that the model might be unintentionally relying on location and time information instead of learning meaningful patterns from sensor signals. I have also experimented with different models, including traditional machine learning and deep learning approaches, but the issue persists. I suspect the problem may be related to the variance of the target PCI values across datasets, potential data leakage caused by overlapping windows, or possibly a methodological flaw in how the evaluation is performed. I want to understand whether it is common in research to report only the R¬≤ values on the train/test splits from the same dataset, or whether researchers typically validate on entirely separate held-out sets as well. Given that classification on the same data works fine but regression fails to generalize, I am trying to figure out if this is expected behavior in PCI regression tasks or if I need to reconsider my entire evaluation strategy.",MachineLearning,Sami10644,0,0.5,9,1757444072.0,https://www.reddit.com/r/MachineLearning/comments/1ncrkpp/d_negative_r¬≤_on_unseen_dataset_despite_good/,https://reddit.com/r/MachineLearning/comments/1ncrkpp/d_negative_r¬≤_on_unseen_dataset_despite_good/,Project,True,False,False,False,False,,2025-09-10T22:30:54.820830+00:00
1nc6r7l,[Project] Otters ü¶¶ - A minimal vector search library with powerful metadata filtering,"I'm excited to share something I've been working on for the past few weeks:

Otters ü¶¶ - A minimal vector search library with powerful metadata filtering powered by an ergonomic Polars-like expressions API written in Rust!

Why I Built This

In my day-to-day work, I kept hitting the same problem. I needed vector search with sophisticated metadata filtering, but existing solutions were either,
Too bloated (full vector databases when I needed something minimal for analysis)
Limited in filtering capabilities
Had unintuitive APIs that I was not happy about.

I wanted something minimal, fast, and with an API that feels natural - inspired by Polars, which I absolutely love.

What Makes Otters Different

Exact Search: Perfect for small-to-medium datasets (up to ~10M vectors) where accuracy matters more than massive scale.

 Performance: 
SIMD-accelerated scoring
Zonemaps and Bloom filters for intelligent chunk pruning

Polars-Inspired API: Write filters as simple expressions
```
meta_store.query(query_vec, Metric::Cosine)
    .meta_filter(col(""price"").lt(100) & col(""category"").eq(""books""))
    .vec_filter(0.8, Cmp::Gt)
    .take(10)
    .collect()
```

The library is in very early stages and there are tons of features that i want to add
Python bindings, NumPy support
Serialization and persistence
Parquet / Arrow integration
Vector quantization
etc.

I'm primarily a Python/JAX/PyTorch developer, so diving into rust programming has been an incredible learning experience.

If you think this is interesting and worth your time, please give it a try.
I welcome contributions and feedback !

üì¶ https://crates.io/crates/otters-rs
üîó https://github.com/AtharvBhat/otters",MachineLearning,AtharvBhat,15,0.94,3,1757383538.0,https://www.reddit.com/r/MachineLearning/comments/1nc6r7l/project_otters_a_minimal_vector_search_library/,https://reddit.com/r/MachineLearning/comments/1nc6r7l/project_otters_a_minimal_vector_search_library/,Project,True,False,False,False,False,,2025-09-10T22:30:55.825851+00:00
1ncyqf9,[D] Metacognitive prompting improves AbstentionBench performance by 10 points,"Hey r/MachineLearning,

I'm not a researcher, just someone interested in LLMs, but I stumbled on something that might be of interest to this community.

TL;DR: By prompting Claude (the only model I tried) to engage in explicit metacognition about its own knowledge limitations, I achieved 70% accuracy on AbstentionBench (Parrish et al., 2025) compared to their reported ~60% baseline. The model showed interesting patterns in where it chose to abstain vs. answer.

Background: AbstentionBench tests whether LLMs know when to say ""I don't know."" it includes questions with false premises, unknowable answers, and context-dependent ambiguities. The benchmark paper shows that scaling and reasoning-focused training make models worse at abstention, degrading performance by 24%.

Method: Instead of treating this as a pure patternmatching task, I prompted the model to: Consider its own knowledge boundaries. Engage in metacognitive evaluation of confidence levels. Acknowledge uncertainty as a valid response. Think about whether it actually knows something vs. can generate something plausible.

Essentially, I framed the task as requiring genuine self-monitoring rather than next-token prediction.

Results: Paper's reported baseline: ~60% With metacognitive prompting: 70% agreement with benchmark labels. The model (Claude) performed near-perfectly on false premise questions (core abstention use case). There was some interesting variation across categories: The model was strong on factual unknowables and false premises. Very conservative on moral/ethical scenarios (abstained on all), and mixed on subjective or ambiguous questions.

Why this might be interesting:

A 10-point improvement from prompting alone suggests abstention behavior is quite malleable.

The category-specific patterns might reveal how different types of uncertainty are processed.

Metacognitive prompting appears to increase caution across the board, sometimes matching and sometimes exceeding benchmark expectations.

Caveats: N=1 (just tested on Claude, needs replication on other models) No controlled baseline (comparing to paper's reported numbers) Manual evaluation of a subset of questions (60 questions). Small sample size would benefit from full dataset evaluation.

Questions for the community:

Has anyone else experimented with metacognitive or self-awareness prompting for abstention?

Are there other benchmarks that might show similar prompting effects?

Any suggestions for automating the evaluation to test on the full dataset?

I emailed one of the benchmark authors but haven't heard back yet. I'd be happy to share the specific prompts and methodology if anyone wants to attempt replication (DM me). Even a 10% improvement from prompting strategy alone seems worth investigating further.

Edit: To be clear, I'm not making claims about consciousness or sentience, just that prompting for self-monitoring behavior appears to meaningfully affect abstention patterns.",MachineLearning,Intraluminal,0,0.36,4,1757460912.0,https://www.reddit.com/r/MachineLearning/comments/1ncyqf9/d_metacognitive_prompting_improves/,https://reddit.com/r/MachineLearning/comments/1ncyqf9/d_metacognitive_prompting_improves/,Discussion,True,False,False,False,False,,2025-09-10T22:30:56.830541+00:00
1ncyf2r,[D] Completed Amazon ML Summer School 2025 curious who else attended?,"Hey everyone,  
I just completed¬†**Amazon ML Summer School 2025**¬†üéâ  
It was a month-long program covering a solid range of ML topics¬†***supervised/unsupervised learning, deep neural nets, generative AI & LLMs, RL, and even causal inference***.  
The sessions were intense but super rewarding. I feel like this experience gave me a strong foundation to explore advanced AI research and projects.

Curious if anyone here has also attended and how you re planning to apply what you learned?

https://preview.redd.it/b5ulzuq038of1.png?width=655&format=png&auto=webp&s=c328f24e6b674b9f576cebae727f44a526f185a9

",MachineLearning,United_Intention42,0,0.33,1,1757460069.0,https://www.reddit.com/r/MachineLearning/comments/1ncyf2r/d_completed_amazon_ml_summer_school_2025_curious/,https://reddit.com/r/MachineLearning/comments/1ncyf2r/d_completed_amazon_ml_summer_school_2025_curious/,Discussion,True,False,False,False,False,,2025-09-10T22:30:57.835579+00:00
1nbhqmq,[D] How do you stay current with AI/ML research and tools in 2025? (Cybersec engineer catching up after Transformers),"Hi everyone,

I‚Äôm a cybersecurity and network engineer/sysadmin by profession, but I studied AI/ML quite seriously at university. My knowledge is solid up until around the Transformer era (when attention-based models started becoming central), but I stopped following developments after that.

Now I‚Äôd like to get back into the field and stay current‚Äînot necessarily to publish research, but to understand new architectures, applications, and tools. In cybersecurity, I stay updated through curated blogs, newsletters, and professional communities. I‚Äôd like to adopt a similar approach for ML/AI.

For those of you who actively track progress:

* Which blogs, newsletters, or feeds do you find most useful?
* Are there particular researchers or labs whose updates you follow?
* Any books or surveys that bridge foundational knowledge with current trends?
* How do you cut through hype-heavy content and focus on signal?

I‚Äôd really appreciate hearing what works for you. The field moves incredibly fast, and I‚Äôd like to plug back in with a structured approach.

Thanks in advance!",MachineLearning,Set-New,94,0.93,16,1757318283.0,https://www.reddit.com/r/MachineLearning/comments/1nbhqmq/d_how_do_you_stay_current_with_aiml_research_and/,https://reddit.com/r/MachineLearning/comments/1nbhqmq/d_how_do_you_stay_current_with_aiml_research_and/,Discussion,True,False,False,False,False,,2025-09-10T22:30:58.838015+00:00
1nbsems,[D] AAAI 26 Alignment Track,Does anyone know whether they‚Äôre going to release the Phase 1 rejections today or on September 12?,MachineLearning,Senior-Let-7576,12,0.88,13,1757348869.0,https://www.reddit.com/r/MachineLearning/comments/1nbsems/d_aaai_26_alignment_track/,https://reddit.com/r/MachineLearning/comments/1nbsems/d_aaai_26_alignment_track/,Discussion,True,False,False,False,False,,2025-09-10T22:30:59.843031+00:00
1nc1mxq,[Project] Phishing URL detection with Random Forests and handcrafted features,"**\[Project\] Phishing URL detection with Random Forests on handcrafted features** 

I recently finished a project where I trained and deployed a phishing URL detector using **traditional ML techniques**. The goal was to explore how far a lightweight, interpretable model could go for this problem before moving to deep learning.

**Data & Features**

* Dataset: Combined PhishTank + Kaggle phishing URLs with Alexa top legitimate domains.
* Preprocessing: Removed duplicates, balanced classes, stratified train/test split.
* Features (hand-engineered):
   * URL length & token counts
   * Number of subdomains, ‚Äú@‚Äù usage, hyphens, digits
   * Presence of IP addresses instead of domains
   * Keyword-based flags (e.g., ‚Äúlogin‚Äù, ‚Äúsecure‚Äù)

**Model & Training**

* Algorithm: Random Forest (scikit-learn).
* Training: 80/20 split, 10-fold CV for validation.
* Performance: \~92% accuracy on test data.
* Feature importance: URL length, IP usage, and hyphen frequency were the strongest predictors.

**Takeaways**

* A simple RF + handcrafted features still performs surprisingly well on phishing detection.
* Interpretability (feature importances) adds practical value in a security context.
* Obvious limitations: feature set is static, adversaries can adapt.

**Future work (exploration planned)**

* Gradient boosting (XGBoost/LightGBM) for comparison.
* Transformers or CNNs on raw URL strings (to capture deeper patterns).
* Automating retraining pipelines with fresh phishing feeds.

**Repo:** [https://github.com/saturn-16/AI-Phishing-Detection-Web-App](https://github.com/saturn-16/AI-Phishing-Detection-Web-App)

Would love feedback on:

* What other URL features might improve detection?
* Have people here seen significant gains moving from RF/GBM ‚Üí deep learning for this type of task?",MachineLearning,Acceptable_Army_6472,0,0.43,3,1757369783.0,https://www.reddit.com/r/MachineLearning/comments/1nc1mxq/project_phishing_url_detection_with_random/,https://reddit.com/r/MachineLearning/comments/1nc1mxq/project_phishing_url_detection_with_random/,Project,True,False,False,False,False,,2025-09-10T22:31:00.848012+00:00
1nbisbw,[D] How to Automate parsing of Bank Statement PDFs to extract transaction level data,"I am working on a project where I need to extract transaction data from Bank Statement PDFs. 80% of my working PDFs are digitally generated so to handle those I put the Regex approach, where I first extract the text into a txt file and then run Regex on this data to extract data in a meaningful format \[Date, Particulars, Credit/Debit amount, Balance\]. The challenge is that the Regex approach is brittle, and very sensitive to formats. So every bank requires a new Regex plus any little change in the format tomorrow by the bank will break the pipeline.

I want to make a pipeline which is agnostic to bank-format and is capable of extracting the info from the PDFs. I cannot use any 3rd party APIs as the bank data is sensitive and we want to keep everything on internal servers.

Hence, I have been exploring ways in Open Source models to built this pipeline. After doing some research, I landed on LayoutLMv3 Model which can essentially label the Tokens based on their location on the page so if we are able to train the model on our data it should be able to tag every token on the page and that should do it, but the challenge here is that this model is sensitive to reading order and fails on few bank formats.

Since then I have explored MinerU but that failed as well, it isolated the transaction content table but later failed to extract data in orderly fashion as it could not differentiate between multiple lines of transactions.

Now I am working with YOLOv8 which I am training to identify transaction rows and amount columns using BBox and then I will pull the info from these BBox intersection. But the confidence here is not very high.

Has anyone here faced similar challenge? Can anyone help me with some solution or approach. It would be a great help!

Know that the most of the PDFs don't have any defined table, it's just text hanging in air with lot of whitespace. I need a solve for Scanned PDFs as well \[integrated with OCR\]",MachineLearning,Anmol_garwal,4,0.63,13,1757322434.0,https://www.reddit.com/r/MachineLearning/comments/1nbisbw/d_how_to_automate_parsing_of_bank_statement_pdfs/,https://reddit.com/r/MachineLearning/comments/1nbisbw/d_how_to_automate_parsing_of_bank_statement_pdfs/,Discussion,True,False,False,False,False,,2025-09-10T22:31:01.853057+00:00
1nbr57g,[R] Benchmarking an ML service in python,"Recently, I needed to build an ML service that would be called by a latency-sensitive client. The requirements for load and latency were higher than what I had worked with in the past, so I wasn‚Äôt sure what to expect from my Python application.

I googled around and couldn‚Äôt find any concrete answers, so I wrote this brief article for anyone out there in a similar situation:

https://medium.com/@javiermas/benchmarking-an-ml-service-in-pytho-4238399d2229

I hope you find it useful!
",MachineLearning,Technical-Seesaw9383,0,0.33,1,1757346058.0,https://www.reddit.com/r/MachineLearning/comments/1nbr57g/r_benchmarking_an_ml_service_in_python/,https://reddit.com/r/MachineLearning/comments/1nbr57g/r_benchmarking_an_ml_service_in_python/,Research,True,False,False,False,False,,2025-09-10T22:31:02.858007+00:00
1naz0eb,[D] Vibe-coding and structure when writing ML experiments,"Hey!

For context, I'm a Master's student at ETH Z√ºrich. A friend and I recently tried writing a paper for a NeurIPS workshop, but ran into some issues.  
We had both a lot on our plate and probably used LLMs a bit too much. When evaluating our models, close to the deadline, we caught up on some bugs that made the data unreliable. We also had plenty of those bugs along the way. I feel like we shot ourselves in the foot but that's a lesson learned the way. Also, it made me realise the negative effects it could have had if those bugs had been kept uncaught.

I've been interning in some big tech companies, and so I have rather high-standard for clean code. Keeping up with those standards would be unproductive at our scale, but I must say I've struggled finding a middle ground between speed of execution and code's reliability.

For researchers on this sub, do you use LLMs at all when writing ML experiments? If yes, how much so? Any structure you follow for effective experimentation (writing (ugly) code is not always my favorite part)? When doing experimentation, what structure do you tend to follow w.r.t collaboration?

Thank you :)

",MachineLearning,Lestode,18,0.65,29,1757265556.0,https://www.reddit.com/r/MachineLearning/comments/1naz0eb/d_vibecoding_and_structure_when_writing_ml/,https://reddit.com/r/MachineLearning/comments/1naz0eb/d_vibecoding_and_structure_when_writing_ml/,Discussion,True,False,False,False,False,,2025-09-10T22:31:03.863044+00:00
1namvsk,Why Language Models Hallucinate - OpenAi pseudo paper - [D],"Hey
Anybody read this ? It seems rather obvious and low quality, or am I missing something ? 

https://openai.com/index/why-language-models-hallucinate/

‚ÄúAt OpenAI, we‚Äôre working hard to make AI systems more useful and reliable. Even as language models become more capable, one challenge remains stubbornly hard to fully solve: hallucinations. By this we mean instances where a model confidently generates an answer that isn‚Äôt true. Our new research paper‚Å†(opens in a new window) argues that language models hallucinate because standard training and evaluation procedures reward guessing over acknowledging uncertainty.
ChatGPT also hallucinates. GPT‚Äë5 has significantly fewer hallucinations especially when reasoning‚Å†, but they still occur. Hallucinations remain a fundamental challenge for all large language models, but we are working hard to further reduce them.‚Äù",MachineLearning,OkOwl6744,106,0.91,48,1757229675.0,https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf,https://reddit.com/r/MachineLearning/comments/1namvsk/why_language_models_hallucinate_openai_pseudo/,Discussion,False,False,False,False,False,,2025-09-10T22:31:04.864345+00:00
1naejuk,[D] The apparent randomness of residual block design,"Skip connections and residual blocks have been ubiquitous in the ML field ever since the original ResNets were published. I think it's fair to say most people agree skip connections help, but at a glance, the design of the residual blocks themselves is still something that differs from paper to paper.

The most recent ""innovation"" is splitting channel mixing from spatial mixing, which is what ConvNeXt does in an attempt to mimic transformers. Other models that also claim SotA-ish performance, however, do not necessarily follow suit. NFNet, for example, employs grouped 3x3 convolution layers, good old normal bottlenecks (not inverted) and channel attention (Squeeze-and-Excitation).

If we look at modern LLMs, they all have residual blocks that look very similar, but with one or two minor differences that often look arbitrary.

I think residual block design is one of those things that people don't really pay much attention to since it generally works well enough regardless of what you do, but at some point it does look like we're just making semi-random decisions based on semi-random observations. Why the block is designed in the way it is is rarely a point of concern.

I've tried looking for papers making direct comparisons between different design choices, but I couldn't really find anything conclusive.

",MachineLearning,Artoriuz,70,0.97,8,1757202709.0,https://www.reddit.com/r/MachineLearning/comments/1naejuk/d_the_apparent_randomness_of_residual_block_design/,https://reddit.com/r/MachineLearning/comments/1naejuk/d_the_apparent_randomness_of_residual_block_design/,Discussion,True,False,False,False,False,,2025-09-10T22:31:05.865321+00:00
1nanw9i,[P] Terra Code CLI ‚Äì An AI coding assistant with domain knowledge and semantic code search,"One limitation I‚Äôve noticed with most AI coding assistants is that they don‚Äôt really understand a team‚Äôs domain knowledge or architectural decisions.

To explore this, we built a small CLI project: Terra Code CLI. The idea was to see if an assistant could feel more like a senior developer who knows the org, rather than just autocomplete.

Things we experimented with:
‚Ä¢ Interactive Knowledge Transfer ‚Äì let senior devs ‚Äúteach‚Äù patterns
‚Ä¢ Semantic Code Search ‚Äì context-aware retrieval across repos
‚Ä¢ Persistent Memory ‚Äì standards remembered across projects
‚Ä¢ Domain Expertise ‚Äì ingesting architecture docs, API specs, etc.

We‚Äôre curious:
üëâ Has anyone here tried giving AI assistants persistent org-specific knowledge? Did it actually help productivity, or just add complexity?

For free quick start:

npm install -g @terra-code/terra-code

terra

For those interested, we‚Äôve open-sourced the CLI [ https://github.com/TerraAGI/terra-code-cli ]. There‚Äôs also a simple website which we will be updating with docs + install guide here: [ https://terra-agi.com/ ]. Currently in beta, so it‚Äôs free to use.",MachineLearning,prabhjots665,4,0.67,1,1757233614.0,https://www.reddit.com/r/MachineLearning/comments/1nanw9i/p_terra_code_cli_an_ai_coding_assistant_with/,https://reddit.com/r/MachineLearning/comments/1nanw9i/p_terra_code_cli_an_ai_coding_assistant_with/,Project,True,False,False,False,False,,2025-09-10T22:31:06.870349+00:00
1nauq2g,[P] Fast ML for Funky FX: Using domain inspired models for embedded DSP,,MachineLearning,boscillator,1,1.0,0,1757255546.0,https://buchanan.one/blog/micro-ml-transient-detector/,https://reddit.com/r/MachineLearning/comments/1nauq2g/p_fast_ml_for_funky_fx_using_domain_inspired/,Project,False,False,False,False,False,,2025-09-10T22:31:07.875311+00:00
1nb5j8f,[P] I Trained an AI to play Donkey Kong Country Stop and Go Station,"Link to github project [Github DK1 Go and Stop Station](https://github.com/paulo101977/DonkeyKongCountry-Stable-and-Go-Station-Reinforcement-Learning/tree/master)

And don't forget to follow our training environment project for PS2 games and others with OpenGL support. This week, I'll be implementing audio capture and framerating for training video recordings:  
[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)",MachineLearning,AgeOfEmpires4AOE4,0,0.27,0,1757280797.0,https://youtube.com/watch?v=j0tC4ligtUc&si=01FrvzDl95ZathKC,https://reddit.com/r/MachineLearning/comments/1nb5j8f/p_i_trained_an_ai_to_play_donkey_kong_country/,Project,False,False,False,False,False,,2025-09-10T22:31:08.880340+00:00
1nahnmz,[D] Thought experiment: ‚ÄúRolling without slipping‚Äù as a blueprint for nD‚Üí(n‚àí1) embeddings?,"I came across the recent ROLLING HONED paper (designing 3D shapes that, when rolling without slipping, trace arbitrary 2D paths). It got me thinking:

In 3D, rolling constraints let you encode a 2D trajectory into the geometry of a 3D body.

In principle, in 4D you could imagine a convex hypersurface rolling on a 3D hyperplane, tracing out a 3D trajectory.

More generally: could there be a systematic way to map nD data into (n‚àí1)D dynamics via such constraints?

I know in ML we already have PCA, autoencoders, product quantization, etc. ‚Äî and those actually preserve metrics we care about. My hunch is that this ‚Äúmechanical embedding‚Äù idea probably fails the usefulness test for similarity search (no guarantee of inner product preservation).

But still:

Does the analogy make any theoretical sense in higher dimensions (rolling manifolds w/o slip/twist)?

Could there be hidden value in treating ‚Äúconstrained dynamics‚Äù as a new kind of coding scheme?

Or am I over-romanticizing a neat geometric trick after too much late-night reading?

Curious what the community thinks ‚Äî is there any research potential here, or should I file this under ‚Äúfun alcohol-fueled metaphors‚Äù and move on?
",MachineLearning,absurdistonvacation,6,0.69,5,1757211884.0,https://www.reddit.com/r/MachineLearning/comments/1nahnmz/d_thought_experiment_rolling_without_slipping_as/,https://reddit.com/r/MachineLearning/comments/1nahnmz/d_thought_experiment_rolling_without_slipping_as/,Discussion,True,False,False,False,False,,2025-09-10T22:31:09.885434+00:00
1n9k5e9,[D] An ML engineer's guide to GPU performance,"My colleague at Modal has been expanding his magnum opus: a beautiful, visual, and most importantly, understandable, guide to GPUs:¬†[https://modal.com/gpu-glossary](https://modal.com/gpu-glossary)

He recently added a whole new section on understanding¬†[GPU performance metrics](https://modal.com/gpu-glossary/perf). Whether you're  
just starting to learn what GPU bottlenecks exist or want to figure out how to speed up your inference or training workloads, there's something here for you.

https://preview.redd.it/bh83nd3lifnf1.png?width=1080&format=png&auto=webp&s=c3507248f585445ae3882e742008c4f54fc5b7e4",MachineLearning,crookedstairs,337,0.99,31,1757114202.0,https://www.reddit.com/r/MachineLearning/comments/1n9k5e9/d_an_ml_engineers_guide_to_gpu_performance/,https://reddit.com/r/MachineLearning/comments/1n9k5e9/d_an_ml_engineers_guide_to_gpu_performance/,Discussion,True,False,False,False,False,,2025-09-10T22:31:10.890395+00:00
1n9spn2,[D] Advice on handling completely incorrect review?,"Recently submitted a paper to WACV 2026. Two of the three reviews are positive. The third recommends rejection, citing items as ‚Äúmissing‚Äù that are actually in the paper (2nd page dude) and claiming our architecture is identical to a 2022 model, though there are clear differences- moreover, the performances tend to drastically differ as showcased in the results.

What are the typical options in this situation? He seems to be inclined towards finding ""excuses"" for rejecting paper (not sure why) and thereby I doubt a rebuttal will help. Can I ask the AC to get the reviewer replaced?",MachineLearning,Forsaken-Order-7376,16,0.81,6,1757141135.0,https://www.reddit.com/r/MachineLearning/comments/1n9spn2/d_advice_on_handling_completely_incorrect_review/,https://reddit.com/r/MachineLearning/comments/1n9spn2/d_advice_on_handling_completely_incorrect_review/,Discussion,True,False,False,False,False,,2025-09-10T22:31:11.895290+00:00
1na7ied,[p] Why per row context understanding is important for data transformations and here's how you can use LLMs to do so,"I had a customers.csv, with columns including names, countries, email id, phone numbers, etc.

I wanted to anonymize all the data that contained personally identifiable information of women, in the dataset.

If you give chatgpt or traditional RAG or SQL databases a large dataset and ask to perform this task, it will execute either a SQL query or a code which will be based on conditional extraction, but for the above task, we need to understand the context, which means the transformation should be aware of names that are female names!

We hacked together a solution for this and here's the example notebook:

[https://github.com/vitalops/datatune/blob/main/examples/data\_anonymization.ipynb](https://github.com/vitalops/datatune/blob/main/examples/data_anonymization.ipynb)",MachineLearning,metalvendetta,0,0.45,0,1757184667.0,https://www.reddit.com/r/MachineLearning/comments/1na7ied/p_why_per_row_context_understanding_is_important/,https://reddit.com/r/MachineLearning/comments/1na7ied/p_why_per_row_context_understanding_is_important/,Project,True,False,False,False,False,,2025-09-10T22:31:12.900326+00:00
1na5ixj,[D]Baseten raises $150M Series D for inference infra. where‚Äôs the real bottleneck?,"Baseten just raised $150M Series D at a $2.1B valuation. They focus on inference infra  like low latency serving, throughput optimization, developer experience.

They‚Äôve shared benchmarks showing their embeddings inference outperforms vLLM and TEI, especially on throughput and latency. The bet is that inference infra is the pain point, not training.

But this raises a bigger question. what‚Äôs the real bottleneck in inference?
	‚Ä¢Baseten and others (Fireworks, Together) are competing on latency + throughput.
	‚Ä¢Some argue the bigger cost sink is cold starts and low GPU utilization , serving multiple models elastically without waste is still unsolved at scale.

I wonder what everyone thinks 

	‚Ä¢Will latency/throughput optimizations be enough to differentiate?
	‚Ä¢Or is utilization (how efficiently GPUs are used across workloads) the deeper bottleneck?
	‚Ä¢Does inference infra end up commoditized like training infra, or is there still room for defensible platforms?
",MachineLearning,pmv143,0,0.48,8,1757179986.0,https://www.reddit.com/r/MachineLearning/comments/1na5ixj/dbaseten_raises_150m_series_d_for_inference_infra/,https://reddit.com/r/MachineLearning/comments/1na5ixj/dbaseten_raises_150m_series_d_for_inference_infra/,Discussion,True,False,False,False,False,,2025-09-10T22:31:13.905249+00:00
1n9ufsf,[P] Knowledge Distillation for Text-to-SQL ‚Äî Training GPT-2 with Qwen2-7B as Teacher,"Hey folks,

I‚Äôve been working on an experiment that combines¬†**Knowledge Distillation (KD)**¬†with the¬†**Text-to-SQL problem**, and I wanted to share the results + repo with the community.

# üéØ Motivation

* Natural language ‚Üí SQL is a powerful way for¬†**non-technical users**¬†to query databases without always relying on analysts.
* Most solutions use massive LLMs (GPT-4.1, etc.), but they‚Äôre¬†**expensive**,¬†**hard to deploy locally**, and raise¬†**data privacy concerns**.
* So the question I asked:¬†*Can a much smaller model (like GPT-2) be trained to generate SQL for a given DB effectively if it learns from a bigger LLM?*

# üß† Approach

I used¬†**Knowledge Distillation (KD)**¬†‚Äî i.e., transferring knowledge from a large teacher model into a smaller student model.

* **Teacher Model**:¬†[Qwen2-7B]()
* **Student Model**:¬†[GPT-2]()

Steps:

1. Built a¬†**custom dataset**¬†‚Üí pairs of (natural language query, SQL query) for a toy retail database schema.
2. Teacher (Qwen2-7B) generates SQL from the queries.
3. Student (GPT-2) is trained on two signals:
   * **Cross-Entropy Loss (75%)**¬†‚Üí match ground-truth SQL.
   * **MSE Loss (25%)**¬†‚Üí align with the teacher‚Äôs hidden state values (projected from teacher‚Äôs layer 25).
4. Trained for¬†**20 epochs on Colab GPU**.

# ‚öôÔ∏è Training Setup

* Teacher hidden states projected ‚Üí aligned with GPT-2‚Äôs final hidden states.
* Loss =¬†**0.75 \* CE + 0.25 \* MSE**.
* Achieved¬†**total loss \~0.21**¬†after training.

# üìä Results

* GPT-2 (student) was able to¬†**generate SQL queries directly from natural language**¬†for the schema.
* While not perfect (due to limited resources at my disposal), it showed that **small models can be viable for domain-specific SQL generation**¬†when trained this way.
* Benefits:
   * ‚ö° Lightweight (runs locally).
   * üí∏ Cost-efficient.
   * üîê More privacy-friendly than cloud-only LLM APIs.

# üì∑ Visuals in the repo:

* Schema diagram (retail DB).
* Teacher ‚Üí Student distillation architecture.
* Sample outputs (NL ‚Üí SQL).

# üìé Repo

Code + diagrams + outputs are here:  
üëâ¬†[GitHub: Knowledge Distillation for SQL generation on GPT-2](https://github.com/Gokul-GMenon/Knowledge_Distillation-SQL_generation_on_gpt_2?utm_source=chatgpt.com)

Would love feedback, suggestions, or discussions on:

* Other lightweight models worth trying as students (LLaMA-7B distilled further? Phi-2?).
* Improvements to the KD setup (layer selection, different projection strategies).
* Extensions: applying this to more complex schemas / real enterprise DBs.

Cheers!

  
Can follow me in [LinkedIn](https://www.linkedin.com/in/gokul-g-menon/) as well for discussions",MachineLearning,Confident-Meal3457,4,0.61,4,1757147760.0,https://www.reddit.com/r/MachineLearning/comments/1n9ufsf/p_knowledge_distillation_for_texttosql_training/,https://reddit.com/r/MachineLearning/comments/1n9ufsf/p_knowledge_distillation_for_texttosql_training/,Project,True,False,False,False,False,,2025-09-10T22:31:14.910231+00:00
1n9wnel,[P] An Open-Source Pipeline for Speech-to-Speech Translation with Voice Preservation (RVC) and Lip-Sync,"Hello¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/),

I'm a final-year undergrad exploring multimodal systems, and I wanted to share a project I've built and open-sourced. It‚Äôs an end-to-end pipeline designed to tackle video dubbing for low-resource languages, using Telugu as the initial target. The system translates speech from an English video while preserving the original speaker's vocal identity and syncing their lips to the new audio.

* **GitHub Repo:**¬†[\[GitHub\]](https://github.com/M-SRIKAR-VARDHAN/speech-to-speech-with-lipsync)
* **Full Technical Write-up:**¬†[\[writeup\]](https://medium.com/@srikarvardhan2005/speech-to-speech-translation-with-lip-sync-425d8bb74530)
* **Demo Video:**¬†[\[Demo\]](https://drive.google.com/drive/folders/1l6jZEDdmUzr9VhfYkvoVdaXJSSipN-nm?usp=sharing)

The core technical challenge was achieving voice preservation without access to large, speaker-specific datasets typically required for high-fidelity voice cloning. After a dead-end attempting a direct S2S architecture inspired by Translatotron, I found that using Retrieval-based Voice Conversion (RVC) as a post-processing step on a generic TTS output was a surprisingly practical and data-efficient solution.

The final pipeline is structured as follows:

1. **ASR:**¬†Whisper for robust transcription.
2. **NMT:**¬†Meta's NLLB for English-to-Telugu translation.
3. **TTS:**¬†Meta's MMS model to synthesize the base Telugu audio.
4. **Voice Conversion:**¬†A trained RVC model converts the timbre of the synthetic speech to match the original speaker.
5. **Lip Sync:**¬†Wav2Lip aligns the video frames to the new audio.

My main takeaway is that RVC seems to function as a very effective ""style transfer"" layer for voice, making it a viable tool for projects where full voice cloning is computationally or data-prohibitive.

I'm sharing this to start a discussion and get feedback from the community on this approach. I'm particularly curious about two points:

1. Has anyone else experimented with using RVC in a more formal pipeline, and what were the qualitative limitations you encountered?
2. Are there newer or more robust alternatives to Wav2Lip for lip-syncing that maintain good performance without requiring massive computational resources?

Any thoughts on the architecture or suggestions for improvement would be highly appreciated. Thank you for your time.",MachineLearning,Nearby_Reaction2947,2,1.0,3,1757156231.0,https://www.reddit.com/r/MachineLearning/comments/1n9wnel/p_an_opensource_pipeline_for_speechtospeech/,https://reddit.com/r/MachineLearning/comments/1n9wnel/p_an_opensource_pipeline_for_speechtospeech/,Project,True,False,False,False,False,,2025-09-10T22:31:15.914780+00:00
1n9xg20,[D] Online hierarchical clustering for news: how to keep event IDs stable under merges/splits in a streaming pipeline?,"I‚Äôm building a news ingestion system (currently Poland-focused; designed to scale) that clusters incoming articles into ‚Äúevents‚Äù powering maps and graph views. Pipeline: embeddings ‚Üí cosine HAC with a fixed threshold ‚Üí periodic (5min) recluster. Granularity, time decay, and summarization are fine, my sole pain point is¬†*stable event identity*¬†in a streaming setting.

As new articles arrive, clusters should sometimes merge (a legitimate bridge appears) or split (bridge was spurious). I need user-facing event IDs to persist through these transitions, i.e., minimize label churn across snapshots while respecting the hierarchical/threshold constraints.

**Question:**¬†What‚Äôs the best-known algorithmic approach (and any open-source references) for¬†*evolutionary/streaming hierarchical clustering with persistent labels*, explicitly merge/split-aware, that¬†*minimizes an inter-snapshot ID-churn* *penalty*¬†under latency constraints?",MachineLearning,local___host,0,0.5,1,1757158986.0,https://www.reddit.com/r/MachineLearning/comments/1n9xg20/d_online_hierarchical_clustering_for_news_how_to/,https://reddit.com/r/MachineLearning/comments/1n9xg20/d_online_hierarchical_clustering_for_news_how_to/,Discussion,True,False,False,False,False,,2025-09-10T22:31:16.919690+00:00
1n9hnq9,[D] Anyone successful with training LoRA for visual LLMs on a multi-GPU setup?,"Hello sub,

I'm trying to train a LoRA for Llama 3.2 90B Visual Instruct on a 8xA100 cluster but I cannot find a framework/package that supports it.

Model is of course too large to fit into a single A100, so the only way is to leverage multiple device.

Unsloth does not support multi GPU training (at least in its open version)  
Axtol has multimodal models in beta

Was any of you successful into training multimodal models of this size? I'd appreciate any kind of feedback.",MachineLearning,KeyIsNull,14,1.0,9,1757107846.0,https://www.reddit.com/r/MachineLearning/comments/1n9hnq9/d_anyone_successful_with_training_lora_for_visual/,https://reddit.com/r/MachineLearning/comments/1n9hnq9/d_anyone_successful_with_training_lora_for_visual/,Discussion,True,False,False,False,False,,2025-09-10T22:31:17.924716+00:00
1n9ecmj,[D]  Anyone attending EUSIPCO next week?,"Anyone attending EUSIPCO in Palermo next week? Unfortunately, none of my labmates will be able to travel, so would be cool to meet new people from here !",MachineLearning,DeeplyConvoluted,7,1.0,3,1757099947.0,https://www.reddit.com/r/MachineLearning/comments/1n9ecmj/d_anyone_attending_eusipco_next_week/,https://reddit.com/r/MachineLearning/comments/1n9ecmj/d_anyone_attending_eusipco_next_week/,Discussion,True,False,False,False,False,,2025-09-10T22:31:18.929711+00:00
1n947jj,[P] I Was Wrong About Complex ML Solutions - Gower Distance Beat My UMAP Approach,"Four years ago, I built [DenseClus ](https://github.com/awslabs/amazon-denseclus)for mixed-data clustering using dual UMAP embeddings. After reflecting on the Zen of Python (""simple is better than complex""), I realized I was overengineering.

Gower (1971) computes distances for mixed categorical/numerical data using weighted averages of appropriate metrics. Despite being 50+ years old, it often outperforms complex embeddings for small-to-medium datasets.

The implementation I coded (with Claude's help) saw a 20% speedup, 40% in memory, has GPU support (CuPy) and Sklearn integration.

Code: [https://github.com/momonga-ml/gower-express](https://github.com/momonga-ml/gower-express)

Blog post with analysis: [https://charles-frenzel.medium.com/i-was-wrong-start-simple-then-move-to-more-complex-5e2f40765481](https://charles-frenzel.medium.com/i-was-wrong-start-simple-then-move-to-more-complex-5e2f40765481)

**Discussion**:  When do you choose simple, interpretable methods over deep embeddings? Have others found similar success reverting to classical approaches?",MachineLearning,Pitiful-Ad8345,17,0.68,12,1757076094.0,https://www.reddit.com/r/MachineLearning/comments/1n947jj/p_i_was_wrong_about_complex_ml_solutions_gower/,https://reddit.com/r/MachineLearning/comments/1n947jj/p_i_was_wrong_about_complex_ml_solutions_gower/,Project,True,False,False,False,False,,2025-09-10T22:31:19.934704+00:00
1n95etu,"[D] Reversed born again network because it's easier to train, is this stupid?","I want to implement this paper: [https://arxiv.org/pdf/1805.04770](https://arxiv.org/pdf/1805.04770)

but I'm not excited about having to manage the student models / save them independently and also there's the issue of cost because we'd have to train each student model from scratch.

To get around this I was thinking I could just do the inverse: train the teacher model and derive ""dark knowledge"" based on the ""incorrect"" logits of the last checkpoint.

What I mean is can I have a training loop similar to the following

    for epoch in range(10):
      student = teacher.clone()
      student.requires_grad_(False) # the student deliberately does not learn, only the teacher learns
      for data in dataset:
        optim.zero_grad()
        teacher_logits = teacher(data.input)
        student_logits = student(data.input)
        loss_cross_entropy = cross_entropy(teacher_logits, data.label)
        loss_dark_knowledge = cross_entropy(teacher_logits - student_logits, data.label)
        loss = (loss_cross_entropy + loss_dark_knowledge) / 2
        loss.backward()
        optim.step()

is this dumb?",MachineLearning,Says_Watt,4,0.7,3,1757079205.0,https://www.reddit.com/r/MachineLearning/comments/1n95etu/d_reversed_born_again_network_because_its_easier/,https://reddit.com/r/MachineLearning/comments/1n95etu/d_reversed_born_again_network_because_its_easier/,Discussion,True,False,False,False,False,,2025-09-10T22:31:20.939710+00:00
1n8lvz5,[D] How do you read code with Hydra,"[Hydra](https://hydra.cc/) has become a very popular in machine learning projects. I understand the appeal, it makes configurations modular, allows you to reuse some parts of it while changing another. It makes the code more reusable and modular too and if you understand all of it its better structured.

My big problem is it makes it damn well near impossible to read someone else's code since every part of the code is now some mysterious implicit thing that gets instantiated from a string in the config file during execution. The problem would be alleviated if there was a way of quickly accessing the definition of the object that will get instantiated at runtime at least with the default values of the config. Is there a plugin that does that? If not, how do you guys do it ?",MachineLearning,Infinite_Explosion,83,0.94,34,1757019202.0,https://www.reddit.com/r/MachineLearning/comments/1n8lvz5/d_how_do_you_read_code_with_hydra/,https://reddit.com/r/MachineLearning/comments/1n8lvz5/d_how_do_you_read_code_with_hydra/,Discussion,True,False,False,False,False,,2025-09-10T22:31:21.944731+00:00
1n918yb,[P] DCNv2 (Update Compatibility) Pytorch 2.8.0,"Hello Reddit,

Working on several project I had to use the DCNv2 for different models I tweak it a little bit to work under the most recent CUDA version I had on my computer. There is probably some changes to make but currently it seems to work on my models training under CUDA 12.8 + Pytorch 2.8.0 configuration still haven't tested the retrocompatibility if anyone would like to give it a try.

Feel free to use it for training model like YOLACT+, FairMOT or others.

[https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main](https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main)",MachineLearning,CaptainBudy,7,1.0,0,1757066980.0,https://www.reddit.com/r/MachineLearning/comments/1n918yb/p_dcnv2_update_compatibility_pytorch_280/,https://reddit.com/r/MachineLearning/comments/1n918yb/p_dcnv2_update_compatibility_pytorch_280/,Project,True,False,False,False,False,,2025-09-10T22:31:22.949638+00:00
1n8po18,[R] The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs,"Curious what folks think about this paper: [https://arxiv.org/abs/2508.08285](https://arxiv.org/abs/2508.08285)  
  
In my own experience in hallucination-detection research, the other popular benchmarks are also low-signal, even the ones that don't suffer from the flaw highlighted in this work.

Other common flaws in existing benchmarks:

\- Too synthetic, when the aim is to catch real high-stakes hallucinations in production LLM use-cases.

\- Full of incorrect annotations regarding whether each LLM response is correct or not, due to either low-quality human review or just relying on automated LLM-powered annotation.

\- Only considering responses generated by old LLMs, which are no longer representative of the type of mistakes that modern LLMs make.  
  
I think part of the challenge in this field is simply the overall difficulty of proper Evals.  For instance, Evals are much easier in multiple-choice / closed domains, but those aren't the settings where LLM hallucinations pose the biggest concern",MachineLearning,jonas__m,31,0.94,7,1757028636.0,https://www.reddit.com/r/MachineLearning/comments/1n8po18/r_the_illusion_of_progress_reevaluating/,https://reddit.com/r/MachineLearning/comments/1n8po18/r_the_illusion_of_progress_reevaluating/,Research,True,False,False,False,False,,2025-09-10T22:31:23.954612+00:00
1n8ynn2,[P] I Built a Convolutional Neural Network that understands Audio,"Hi everyone, I am sharing a project that I built recently, I trained a convolutional neural network (CNN) based on a¬†ResNet‚Äë34 style residual architecture¬†to classify audio clips from the¬†ESC‚Äë50 dataset¬†(50 environmental sound classes). I used log‚Äìmel spectrograms as input, reached strong accuracy and generalization with residual blocks, and packaged the model with dropout and adaptive average pooling for robustness. Would love to get your opinions on it.  Check it out -->¬†[https://sunoai.tanmay.space](https://sunoai.tanmay.space/)

Read the blog -->¬†[https://tanmaybansal.hashnode.dev/sunoai](https://tanmaybansal.hashnode.dev/sunoai)",MachineLearning,Tanmay__13,3,0.55,12,1757056865.0,https://www.reddit.com/r/MachineLearning/comments/1n8ynn2/p_i_built_a_convolutional_neural_network_that/,https://reddit.com/r/MachineLearning/comments/1n8ynn2/p_i_built_a_convolutional_neural_network_that/,Project,True,False,False,False,False,,2025-09-10T22:31:24.959701+00:00
1n9m5hv,[D] Seeking arXiv endorsement,"Hi All

I‚Äôm preparing to submit to arXiv in Experimentation. Since this is my first submission, I need an endorsement.

The draft is ready and I can share it upon request. Thanks! 
",MachineLearning,Specialist_Clock_368,0,0.12,2,1757119886.0,https://www.reddit.com/r/MachineLearning/comments/1n9m5hv/d_seeking_arxiv_endorsement/,https://reddit.com/r/MachineLearning/comments/1n9m5hv/d_seeking_arxiv_endorsement/,Discussion,True,False,False,False,False,,2025-09-10T22:31:25.964773+00:00
1n83e6e,[D] Performance overhead of running ML inference in hardware-isolated environments - production metrics,"Been collecting data on ML inference performance in trusted execution environments and thought the numbers might be useful for others dealing with similar constraints.

**Context:** Fraud detection models processing ~10M daily transactions, needed hardware-level isolation for compliance reasons.

After 3 months of production data, seeing 5-8% performance overhead compared to standard deployment. This is way better than the 30-40% overhead reported in older papers about SGX.

The interesting technical challenge was memory management. TEE environments have strict memory limits and different allocation patterns than standard containers. Had to completely rewrite our batching logic - what worked fine with dynamic batching in regular pods caused constant OOM errors in enclaves.

**Model optimization discoveries:**

- ONNX runtime worked, pytorch was too memory heavy
- Preprocessing became the bottleneck, not inference
- Had to keep models under 8GB total memory
- P95 latency went from 12ms to 13ms

Tried multiple approaches including raw SGX implementation and phala's abstraction layer. The attestation complexity alone makes raw implementation painful.

**For those working on similar problems:**
Profile your entire pipeline, not just model inference. Data transformation overhead in isolated environments is real.

**Technical question for the community:** 
How are you handling model updates in TEE environments? The attestation requirements make standard blue-green deployments complicated. Currently doing full enclave restarts but that means brief downtime.

Also curious if anyone's tried running transformer models larger than 1B params in TEE. Memory constraints seem prohibitive but maybe there are tricks I'm missing?",MachineLearning,baddie_spotted,2,0.67,0,1756969887.0,https://www.reddit.com/r/MachineLearning/comments/1n83e6e/d_performance_overhead_of_running_ml_inference_in/,https://reddit.com/r/MachineLearning/comments/1n83e6e/d_performance_overhead_of_running_ml_inference_in/,Discussion,True,False,False,False,False,,2025-09-10T22:31:26.969731+00:00
