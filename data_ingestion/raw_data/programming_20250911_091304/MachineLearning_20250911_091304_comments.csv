id,post_id,parent_id,author,body,score,created_utc,permalink,is_submitter,depth,controversiality,distinguished,edited,retrieved_at,subreddit
nbymt7y,1n67lft,1n67lft,parlancex,"I've been training a (custom) video game music diffusion model on a single consumer GPU and improving the model over the last 2 years. The current model has about 5 weeks of training on an RTX 5090.

Demo audio is here: https://www.g-diffuser.com/dualdiffusion/

Code is here: https://github.com/parlance-zz/dualdiffusion

I posted here about a year ago with an older version of the model. The new model is trained on a large variety of modern video game music instead of just Super Nintendo music and includes a variety of architectural changes for a large improvement in audio quality.

Public weights will be available soon (100% free and open), but I think the bigger deal is that it is possible, practical even, to train a viable music diffusion model on consumer desktop hardware. I'm sure there are folks out there with a decent desktop GPU and troves of music that might like the idea of creating their own music model with their data. The code repository has everything you would need to do it from dataset preprocessing to DAE / DDEC and LDM training, and inference.

The github page has a detailed log of all the technical details and improvements made to the model over the last 2 years.",6,1756788123.0,https://reddit.com/r/MachineLearning/comments/1n67lft/d_selfpromotion_thread/nbymt7y/,False,0,0,,False,2025-09-11T13:12:41.008497+00:00,MachineLearning
nc0zhw5,1n67lft,1n67lft,await_void,"I've been working on an **Explainable Vision Language Model** for product defect detection and things turned out great. It doesn't only do that, but using **CLIP** as a backbon it can also **auto label entire dataset** with a knowledge base pool; discovering about Contrastive Learning was a blast. 

  
This is my master thesis project and i had a lot of fun experimenting with multimodal contexts and linking different kind of models between them, it's super fun and mind blowing seeing how different embeddings can link out with each other forming methods such as image captioning, explaining, reasoning. 

For anyone interested, this is my original post: [https://www.reddit.com/r/computervision/comments/1n6llyh/tried\_building\_an\_explainable\_visionlanguage/](https://www.reddit.com/r/computervision/comments/1n6llyh/tried_building_an_explainable_visionlanguage/)

And this is my code repository on GitHub: [https://github.com/Asynchronousx/CLIPCap-XAI/](https://github.com/Asynchronousx/CLIPCap-XAI/)

If you have any comments about the project, feedback or curiosity, ask out!",3,1756826112.0,https://reddit.com/r/MachineLearning/comments/1n67lft/d_selfpromotion_thread/nc0zhw5/,False,0,0,,False,2025-09-11T13:12:41.008594+00:00,MachineLearning
nc1jp5i,1n67lft,nbymt7y,Relative_Listen_6646,Pretry cool work!,2,1756832149.0,https://reddit.com/r/MachineLearning/comments/1n67lft/d_selfpromotion_thread/nc1jp5i/,False,1,0,,False,2025-09-11T13:12:41.008645+00:00,MachineLearning
ndisfy1,1ndo5md,1ndo5md,Majromax,"The cost-effectiveness depends on your cost structure.

If your biggest worry is the cost of power, then Blackwell Ultra's utility will come down to its FLOPS per watt.  Idle GPUs draw an order of magnitude less energy than busy ones.

If your biggest worry is latency rather than throughput, Blackwell Ultras might be worth the cost even if they sit idle.  If you're a hedge fund competing for the last microsecond, for example, then you want to climb far up the 'inefficiency' curve for your edge.

If your computational requirement is roughly fixed, then more powerful GPUs might also let you consolidate the total number of systems.  You might end up saving on other infrastructure costs.

Finally, if your main worry is about the amortized capital costs of the cards themselves, then Blackwell Ultra probably isn't worth it.  However, _no_ new release is probably worth it on that basis; why aren't you buying used A100s?",18,1757538957.0,https://reddit.com/r/MachineLearning/comments/1ndo5md/dnvidia_blackwell_ultra_crushes_mlperf/ndisfy1/,False,0,0,,False,2025-09-11T13:12:46.063696+00:00,MachineLearning
ndkrnk1,1ndo5md,1ndo5md,djm07231,"It could be useful for RL applications.

The bottleneck for RL is waiting for inference rollouts so you will be doing inference constantly.

You will probably come closer to maximum utilization in which case this kind of benchmarks could be more relevant.",4,1757562949.0,https://reddit.com/r/MachineLearning/comments/1ndo5md/dnvidia_blackwell_ultra_crushes_mlperf/ndkrnk1/,False,0,0,,False,2025-09-11T13:12:46.063773+00:00,MachineLearning
ndjdron,1ndo5md,ndisfy1,pmv143,"Good points. One wrinkle is that in practice, workloads aren’t steady . GPUs sit idle a lot of the time, and that undercuts cost-effectiveness no matter how efficient the chip is per watt. Benchmarks capture peak throughput, but the real challenge is keeping GPUs busy in bursty, multi-model environments. That’s often where the economics break down, not just at the hardware level",3,1757545287.0,https://reddit.com/r/MachineLearning/comments/1ndo5md/dnvidia_blackwell_ultra_crushes_mlperf/ndjdron/,True,1,0,,False,2025-09-11T13:12:46.063813+00:00,MachineLearning
ndlau2g,1ndo5md,ndkrnk1,pmv143,"This is exactly the tension we see. If you’re in RL or steady-state inference, raw throughput benchmarks map pretty well to cost. But for most real-world workloads, traffic is bursty, GPUs sit idle, and orchestration across models eats into utilization. That’s why solutions that reduce cold starts and rehydrate GPU state faster end up having as much impact on economics as FLOPS benchmarks do.",2,1757572096.0,https://reddit.com/r/MachineLearning/comments/1ndo5md/dnvidia_blackwell_ultra_crushes_mlperf/ndlau2g/,True,1,0,,False,2025-09-11T13:12:46.063837+00:00,MachineLearning
ndji82j,1ndtey6,1ndtey6,Brudaks,"Cleaning data has always been a major part of the work required in making realistic production pipelines for every data analysis domain, before machine learning it also applies to data science, business intelligence and data warehouses and whatnot. This problem has been around for literally half a century, and is as important as ever.",10,1757546706.0,https://reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/ndji82j/,False,0,0,,False,2025-09-11T13:12:51.052660+00:00,MachineLearning
ndjq75w,1ndtey6,1ndtey6,InternationalMany6,"I’m not sure what you mean by organizing?

Are these malformed CSV’s like with inconsistent rows and columns?

Are you meaning where the files themselves are saved, like if you split them into folders? Or move them into a database?",4,1757549345.0,https://reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/ndjq75w/,False,0,0,,False,2025-09-11T13:12:51.052703+00:00,MachineLearning
ndjo9yl,1ndtey6,1ndtey6,swaneerapids,what makes the csvs unusable?,3,1757548686.0,https://reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/ndjo9yl/,False,0,0,,False,2025-09-11T13:12:51.052716+00:00,MachineLearning
ndjpmfj,1ndtey6,1ndtey6,Xtianus21,You want to convert the CSV into parquet and put it into icetables if you can do that. Snowflake would do this easily for you.,3,1757549148.0,https://reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/ndjpmfj/,False,0,0,,False,2025-09-11T13:12:51.052727+00:00,MachineLearning
ndkeirp,1ndtey6,1ndtey6,CrownLikeAGravestone,"For dealing with power signals, it's common to have them encoded and compressed as waveforms (e.g. in a format you might use for audio) rather than text-like files as CSVs which assists a lot with sampling and transforming. Parquet is another much better option than CSVs if you need tabular data. 

Do what you can to put the CSV data into a better format *fast* \- e.g. on ingestion, make the CSV read/Parquet write program highly optimised and simple in a nice compiled language, then do the scripting/analysis/learning part with the better format IMO.",3,1757557835.0,https://reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/ndkeirp/,False,0,0,,False,2025-09-11T13:12:51.052738+00:00,MachineLearning
ndjjky6,1ndtey6,ndji82j,grabber500,Crap in crap out so its painstaking. What are you using to help. I put together some python scripts but I need a new one for each set of different data we take.,3,1757547150.0,https://reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/ndjjky6/,True,1,0,,False,2025-09-11T13:12:51.052759+00:00,MachineLearning
ndjx0lh,1ndtey6,ndjo9yl,grabber500,The way they are downloaded from the unit. There may be a bit of intentional difficulty in the output CSVs as I cant see any way you could use them anywhere else off the bat. We input the raw data into other software for analysis and training hence we need them in a specific format.,4,1757551703.0,https://reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/ndjx0lh/,True,1,0,,False,2025-09-11T13:12:51.052776+00:00,MachineLearning
ndjsdb3,1ndtey6,ndjrayf,InternationalMany6,"I tend to push any tabular data into formats like parquet or databases like duckdb. Script everything so the man hours are just to double check results and hit the “run” button. 

It’s unclear, are you able to write and optimize code or are you looking for out of the box tools? Nothing wrong with the later but it’s a different question. ",2,1757550092.0,https://reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/ndjsdb3/,False,2,0,,False,2025-09-11T13:12:51.052791+00:00,MachineLearning
ndkg4hx,1ndtey6,ndjx0lh,swaneerapids,"Still difficult to understand what exactly is going on. Sounds like you've got the CSVs coming from the power meter + other measurements that you want to aggregate into timestamped data streams.

You need to write some custom program (python for example) that can take these multiple inputs. For the csv for example you can use:  
\`\`\`  
df = pd.read\_csv(<csv file>, usecols=\[<list of known good columns>\])  
\`\`\`

not sure if the csv you get has timestamps per row...  
if your other inputs are also timestamped, then you can try to find the nearest row in the original csv and append the auxiliary input as a separate row:  
you can use \`ts = pd.to\_datetime(<string value>)\` and then \`.get\_loc(ts, 'nearest')\` etc. pandas has decent timestamping functions.

Finally your program would output the cleaned and merged dataframes into new csvs \`df\_new.to\_csv(<output.csv>)\`

You can then hand off that cleaned/processed csv to the downstream tasks.  
Either way you'll need to write that processing code - so you'll have to consider any edge cases and write methods to handle them. Ideally this would be one program to handle it all.",5,1757558420.0,https://reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/ndkg4hx/,False,2,0,,False,2025-09-11T13:12:51.052804+00:00,MachineLearning
ndhpm6i,1ndaesz,1ndaesz,LelouchZer12,"Try ModernBERT (english only) or EuroBERT, those are the most powerful transformer encoder-only embedding models right now.",2,1757528507.0,https://reddit.com/r/MachineLearning/comments/1ndaesz/d_sota_modern_alternative_to_bertscore/ndhpm6i/,False,0,0,,False,2025-09-11T13:12:53.499223+00:00,MachineLearning
ndh1b36,1ndaesz,ndfqyd7,Jamaleum,"The original BERTScore does find an optimal layer to extract the token embeddings from, which would need to be identified first.",2,1757521677.0,https://reddit.com/r/MachineLearning/comments/1ndaesz/d_sota_modern_alternative_to_bertscore/ndh1b36/,False,1,0,,False,2025-09-11T13:12:53.499268+00:00,MachineLearning
ndlku5v,1ne2qux,1ne2qux,FammasMaz,"Don't know about ML but the biggest going on rn is the project LCRI.

Thats ""Lets crawl reddit for ideas""",6,1757577783.0,https://reddit.com/r/MachineLearning/comments/1ne2qux/d_only_out_of_curiosity/ndlku5v/,False,0,0,,False,2025-09-11T13:12:56.028418+00:00,MachineLearning
ndfa0m6,1ndajmq,1ndajmq,Brudaks,"What you actually need to do is to make a solid case (and convince the reviewers) that the contribution of this paper is something that people should read and use.

If the main contribution of a paper is a new method, then you need to successfully argue that at least some people should and would use that new method in their future work.

Comparing primarily against ""same-family"" methods can be a valid argument if and only if other people are (or should be) using that family - in that case, you're advancing the state of art in that domain with whatever restrictions are pushing people to use methods of that family instead of something else that theoretically gets higher accuracy but has some other disadvantages.

On the other hand, if the reviewers believe that the whole family is outdated and made irrelevant by other methods, then advances to that family wouldn't be useful contributions (unless they're so big that they make the whole class of methods useful again) and the burden of proof lies on your paper to clearly and convincingly convince them otherwise.

So for your specific examples, for an improvement to bagging-based random forests, you have to make a solid case why people should be interested in these improved bagging-based random forests even if they currently think XGBoost is better. One way to make the case is to explicitly compare them against XGBoost (and even if the accuracy is lower, make a clear argument based on other advantages). Another way is to succesfully convince the reader that in particular domain(s) domain bagging-based random forests (and advances to them) are currently very relevant because they can't be replaced with XGBoost in this case (for reasons and evidence which your paper provides) and thus XGBoost can be excluded from comparison.

  
In any case, ""fairness"" doesn't really come in play here. The ""why should people care about this paper"" is an objectively valid filter no matter if it makes hard or impossible for someone to publish - irrelevant things shouldn't be published, the signal-to-noise is too bad as it is.",24,1757501128.0,https://reddit.com/r/MachineLearning/comments/1ndajmq/d_questions_on_fairness_and_expectations_in/ndfa0m6/,False,0,0,,True,2025-09-11T13:13:01.022997+00:00,MachineLearning
ndfffji,1ndajmq,1ndajmq,tfburns,"tl;dr it depends  
  
1. Not necessarily. It could be a theory paper where the contribution is to improve our understanding, for example. You could frame that as ""improving SOTA in terms of understanding"", but I think that's unhelpful and leaning into ""playing the game"" rather than just focussing on the science. That said, different venues and sub-fields have different expectations, so it is hard to say generally what will be considered. And it changes, not only over time but also within the same community -- one set of reviewers might think it's great and another not. So there is also a lot of noise in the system, not to mention background political and funding or commercial interests at play.

2. It depends on what your scientific question is and what claim(s) you are making based on those. To take one of your examples, if you want, you can limit your question's scope to: how do different bagging procedures in random forest perform on datasets XYZ w.r.t. metrics ABC, and does my modification do better? But if you want to get accepted to a particular venue, then you need to ask: is that question interesting to people from that venue? Maybe yes, maybe not (see response to your first question).",4,1757503557.0,https://reddit.com/r/MachineLearning/comments/1ndajmq/d_questions_on_fairness_and_expectations_in/ndfffji/,False,0,0,,False,2025-09-11T13:13:01.023085+00:00,MachineLearning
ndgxm0a,1ndajmq,1ndajmq,choHZ,"1. You don’t need to be SOTA, but having SOTA-competitive performance is one of the main metrics. However, having a fair and more comprehensive experiment is, at least in my view, much more important and informative than simply being SOTA. Many so-called SOTA works cherry-pick datasets and settings (intentionally or not), so what I often do is run a ton of experiments to show that no single work is SOTA in all cases, but (ideally) mine is SOTA or SOTA-competitive in most.

2. The family doesn’t matter, but the characteristics of the family do. For example, if you’re proposing a bagging variant, your main advantage over boosting might be parallelism, which can translate into efficiency gains. But can it really materialize? E.g., if you’re ensembling 10 small models that can each be trained in short hours, then the efficiency-by-parallelism benefit might not be that significant; otherwise, it could be. Just identify metrics that matters, and fairly compare on those metrics.",2,1757520633.0,https://reddit.com/r/MachineLearning/comments/1ndajmq/d_questions_on_fairness_and_expectations_in/ndgxm0a/,False,0,0,,False,2025-09-11T13:13:01.023102+00:00,MachineLearning
ndfzcgl,1ndajmq,ndfa0m6,Entrepreneur7962,"I agree. 
I wish I had these realizations sooner in my career.",2,1757510726.0,https://reddit.com/r/MachineLearning/comments/1ndajmq/d_questions_on_fairness_and_expectations_in/ndfzcgl/,False,1,0,,False,2025-09-11T13:13:01.023134+00:00,MachineLearning
ndcqhgs,1ncu0na,1ncu0na,NamerNotLiteral,"Went from a 3/3/2.5 scores in February to 3/2/1.5 scores now, and the feedback was a lot less substantial and very niypicky by comparison. We got 5-6 actionable points to improve on that time, and only 2 minor points to work on this time. 

*Bleeurgh*

We feel the paper is significantly better organized in its current state compared to the one we submitted in February. 

I don't think it's possible to commit Feb's reviews to AACL with the current paper, is it?",5,1757459773.0,https://reddit.com/r/MachineLearning/comments/1ncu0na/d_ijcnlpaacl_2025_paper_reviews_arr_july_2025/ndcqhgs/,False,0,0,,False,2025-09-11T13:13:03.552804+00:00,MachineLearning
nddjanx,1ncu0na,ndcqhgs,Starscream-11813,"Yes, but I believe you need to justify why the latter set of scores became lower.",2,1757469772.0,https://reddit.com/r/MachineLearning/comments/1ncu0na/d_ijcnlpaacl_2025_paper_reviews_arr_july_2025/nddjanx/,True,1,0,,False,2025-09-11T13:13:03.552885+00:00,MachineLearning
nde7kdh,1ncu0na,nddkc9s,Status-Effect9157,"Yeah, and it's possible that some folks who got a good score this ARR cycle might opt to commit to a latter cycle that coincides with other conferences like NAACL (or even EACL).",2,1757479693.0,https://reddit.com/r/MachineLearning/comments/1ncu0na/d_ijcnlpaacl_2025_paper_reviews_arr_july_2025/nde7kdh/,False,2,0,,False,2025-09-11T13:13:03.552915+00:00,MachineLearning
